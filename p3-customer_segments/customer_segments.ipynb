{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Customer Segments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project you, will analyze a dataset containing annual spending amounts for internal structure, to understand the variation in the different types of customers that a wholesale distributor interacts with.\n",
    "\n",
    "Instructions:\n",
    "\n",
    "- Run each code block below by pressing **Shift+Enter**, making sure to implement any steps marked with a TODO.\n",
    "- Answer each question in the space provided by editing the blocks labeled \"Answer:\".\n",
    "- When you are done, submit the completed notebook (.ipynb) with all code blocks executed, as well as a .pdf version (File > Download as)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset has 440 rows, 6 columns\n",
      "   Fresh  Milk  Grocery  Frozen  Detergents_Paper  Delicatessen\n",
      "0  12669  9656     7561     214              2674          1338\n",
      "1   7057  9810     9568    1762              3293          1776\n",
      "2   6353  8808     7684    2405              3516          7844\n",
      "3  13265  1196     4221    6404               507          1788\n",
      "4  22615  5410     7198    3915              1777          5185\n"
     ]
    }
   ],
   "source": [
    "# Import libraries: NumPy, pandas, matplotlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Tell iPython to include plots inline in the notebook\n",
    "%matplotlib inline\n",
    "\n",
    "# Read dataset\n",
    "data = pd.read_csv(\"wholesale-customers.csv\")\n",
    "print \"Dataset has {} rows, {} columns\".format(*data.shape)\n",
    "print data.head()  # print the first 5 rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1)** In this section you will be using PCA and ICA to start to understand the structure of the data. Before doing any computations, what do you think will show up in your computations? List one or two ideas for what might show up as the first PCA dimensions, or what type of vectors will show up as ICA dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "The first PCA dimension might show a vector with a high absolute value for fresh and milk, because selling milk usually goes together with selling fresh produce. Another possibility for first PCA dimension is a vector with high absolute value for grocery, frozen and detergents paper because those category fits into convenience store that does not sell fresh produce.\n",
    "\n",
    "ICA dimension vector might show a linear combination of fresh and milk to form the first source (latent variable), or a combination of grocery, frozen and detergents paper to form the second source. For example: \n",
    "```\n",
    "s1 = 2*fresh + 1*milk\n",
    "s2 = grocery + frozen + 2*detergents\n",
    "\n",
    "# in vector\n",
    "s1 = [2, 1, 0, 0, 0, 0]\n",
    "s2 = [0, 0, 1, 1, 2, 0]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.97653685 -0.12118407 -0.06154039 -0.15236462  0.00705417 -0.06810471]\n",
      " [-0.11061386  0.51580216  0.76460638 -0.01872345  0.36535076  0.05707921]\n",
      " [-0.17855726  0.50988675 -0.27578088  0.71420037 -0.20440987  0.28321747]\n",
      " [-0.04187648 -0.64564047  0.37546049  0.64629232  0.14938013 -0.02039579]\n",
      " [ 0.015986    0.20323566 -0.1602915   0.22018612  0.20793016 -0.91707659]\n",
      " [-0.01576316  0.03349187  0.41093894 -0.01328898 -0.87128428 -0.26541687]]\n",
      "[ 0.45961362  0.40517227  0.07003008  0.04402344  0.01502212  0.00613848]\n"
     ]
    }
   ],
   "source": [
    "# TODO: Apply PCA with the same number of dimensions as variables in the dataset\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=6)\n",
    "pca.fit(data)\n",
    "\n",
    "# Print the components and the amount of variance in the data contained in each dimension\n",
    "print pca.components_\n",
    "print pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2)** How quickly does the variance drop off by dimension? If you were to use PCA on this dataset, how many dimensions would you choose for your analysis? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Answer:** Looking at the explained variance ratio, after the first 2 primary components, the variance drop off significantly. Based on this result, I would choose 2 dimensions for analysis, because the remaining dimensions' variance is small enough that there would be little information lost by removing them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3)** What do the dimensions seem to represent? How can you use this information?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Answer:** \n",
    "The 1st primary component is a vector with a lot of weight on the Fresh category (-0.98), followed by Frozen (-0.15) and Milk (-0.12). Because frozen and milk value is relatively small compare to the fresh category, this component seem to be driven mostly by the fresh category, and it accounts for most of the variance in the data. \n",
    "The 2nd primary component is a vector with the biggest value on Grocery (0.76), Milk (0.52) and Detergents paper (0.37). This indicates that there is a pretty strong correlation between Grocery, Milk and Detergents paper. The combination of these 3 features made up most of the remaining variance of the data.\n",
    "\n",
    "We can use this information to see what original features contribute to most of the variance in the data. These 2 principal components--that is a mix of fresh, and (grocery, milk, and detergents paper)--account for most of the variance in the data. So we can use these 2 principal components as new features and project the data to reduce the number of features to analyze while still retaining most of the information from the original data. Reducing features will be useful in making calculation simpler and faster. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ICA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Fresh  Milk  Grocery  Frozen  Detergents_Paper  Delicatessen\n",
      "0  12669  9656     7561     214              2674          1338\n",
      "ICA components rounded down to 2 decimal places\n",
      "[[ 0.    0.07 -0.06 -0.    0.02 -0.02]\n",
      " [-0.    0.02  0.11 -0.01 -0.13 -0.02]\n",
      " [-0.    0.01 -0.07 -0.    0.02  0.01]\n",
      " [ 0.    0.    0.01  0.   -0.   -0.05]\n",
      " [-0.05  0.01  0.01  0.   -0.01  0.  ]\n",
      " [ 0.01  0.   -0.01 -0.05  0.    0.02]]\n",
      "ICA components after recentering (so there are no negative values)\n",
      "[[ 0.13  0.2   0.07  0.13  0.15  0.11]\n",
      " [ 0.13  0.15  0.24  0.12  0.    0.11]\n",
      " [ 0.13  0.14  0.06  0.13  0.15  0.14]\n",
      " [ 0.13  0.13  0.14  0.13  0.13  0.08]\n",
      " [ 0.08  0.14  0.14  0.13  0.12  0.13]\n",
      " [ 0.14  0.13  0.12  0.08  0.13  0.15]]\n"
     ]
    }
   ],
   "source": [
    "# TODO: Fit an ICA model to the data\n",
    "# Note: Adjust the data to have center at the origin first!\n",
    "from sklearn.decomposition import FastICA\n",
    "from sklearn import preprocessing\n",
    "\n",
    "ica = FastICA(n_components=6)\n",
    "\n",
    "# mean = data.mean()\n",
    "# print \"mean from original data: \"\n",
    "# print mean\n",
    "print data[0:1]\n",
    "\n",
    "data_centered = preprocessing.scale(data)\n",
    "# print \"data centered around origin:\"\n",
    "# print data_centered[0:5]\n",
    "\n",
    "ica.fit(data_centered)\n",
    "\n",
    "# Print the independent components\n",
    "# Note: rounded down to 3 decimal places for more readability\n",
    "rounded_ica_components = ica.components_.round(2)\n",
    "print \"ICA components rounded down to 2 decimal places\"\n",
    "print rounded_ica_components\n",
    "\n",
    "print \"ICA components after recentering (so there are no negative values)\"\n",
    "recentered_ica_components = rounded_ica_components - rounded_ica_components.min()\n",
    "print recentered_ica_components\n",
    "# print normalized_ica_components[0].sum()\n",
    "# print normalized_ica_components.sum(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4)** For each vector in the ICA decomposition, write a sentence or two explaining what sort of object or property it corresponds to. What could these components be used for?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Answer:**\n",
    "\n",
    "Each vector in the ICA decomposition represent some latent variables (sources) that is composed of linear combination of the observed features (fresh, milk, groceries, etc). \n",
    "\n",
    "```\n",
    "     Fresh  Milk  Groc  Froz  Det   Deli\n",
    "1st [ 0.13  0.2   0.07  0.13  0.15  0.11]\n",
    "2nd [ 0.13  0.15  0.24  0.12  0.    0.11]\n",
    "3rd [ 0.13  0.14  0.06  0.13  0.15  0.14]\n",
    "4th [ 0.13  0.13  0.14  0.13  0.13  0.08]\n",
    "5th [ 0.08  0.14  0.14  0.13  0.12  0.13]\n",
    "6th [ 0.14  0.13  0.12  0.08  0.13  0.15]\n",
    "```\n",
    "\n",
    "The 1st ICA component represent an independent source that is relatively composed of:\n",
    "```\n",
    "0.13 Fresh + 0.2 Milk + 0.07 Grocery + 0.13 Frozen + 0.15 Detergents paper + 0.11 Delicatessen\n",
    "```\n",
    "\n",
    "ICA can be used to transforms the data about spending on the observed categories, such as Fresh, Milk, etc, into spending on, maybe, certain type of products that our customer buy depending on the type of customer. For example, looking at the first ICA, maybe it's a category of product that is used by a deli shop like fresh vegtables, meat, milk, a little grocery (for spices, salt), detergents for washing dishes and paper towel for kitchen and bathroom.    \n",
    "\n",
    "In the same way, the 2nd ICA component represent a source that is relatively composed of:\n",
    "```\n",
    "0.13 Fresh + 0.15 Milk + 0.24 Grocery + 0.12 Frozen + 0.0 Detergents paper + 0.11 Delicatessen\n",
    "```\n",
    "\n",
    "This might represent category of product that is bought by stores that only sell food (very little detergents paper spending).\n",
    "\n",
    "3rd ICA component: \n",
    "```\n",
    "0.13 Fresh + 0.14 Milk + 0.06 Grocery + 0.13 Frozen + 0.15 Detergents paper + 0.14 Delicatessen\n",
    "```\n",
    "\n",
    "This is very similar to the 1st ICA component, with milk and deli swapping some weight.\n",
    "\n",
    "4th ICA component: \n",
    "```\n",
    "0.13 Fresh + 0.13 Milk + 0.14 Grocery + 0.13 Frozen + 0.13 Detergents paper + 0.08 Delicatessen\n",
    "```\n",
    "\n",
    "This seem to represent a category of product that spans all original categories equally, except delicatessen. It might be the type of products purchased by a big grocery store that does not have deli section (or a small deli section). \n",
    "\n",
    "And so on for n-th component. \n",
    "\n",
    "It should be noted that the result from running ICA is always different every time (unstable), making interpretation of this result hard, and probably not very useful, as ICA doesn't seem to be able to find a consistent independent components that underlie the data. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering\n",
    "\n",
    "In this section you will choose either K Means clustering or Gaussian Mixed Models clustering, which implements expectation-maximization. Then you will sample elements from the clusters to understand their significance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose a Cluster Type\n",
    "\n",
    "**5)** What are the advantages of using K Means clustering or Gaussian Mixture Models?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: \n",
    "K Means is simple and fast clustering algorithm for large data set. However it does not always produce a consistent result (its result can be significantly different depending on the initialization state). It also performs poorly when there is significant density difference between the clusters, because the dense cluster pulls the center of less dense cluster (http://varianceexplained.org/r/kmeans-free-lunch/).\n",
    "\n",
    "Gaussian Mixture Models has the advantage of having a soft classification feature, meaning that there can be grey area where the algortihm predicts some points as having close to 50/50 chance of being in one cluster or another. It uses probabilistic approcah so we can inquire the probability of a data point being in one cluster. \n",
    "\n",
    "For this purpose, even though both KMeans and GMM produce similar result and is fast enough for the data set, I chose GMM because it seems to fit the data better visually. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6)** Below is some starter code to help you visualize some cluster data. The visualization is based on [this demo](http://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_digits.html) from the sklearn documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import clustering modules\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.mixture import GMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  -650.02212207   1585.51909007]\n",
      " [  4426.80497937   4042.45150884]\n",
      " [  4841.9987068    2578.762176  ]\n",
      " [  -990.34643689  -6279.80599663]\n",
      " [-10657.99873116  -2159.72581518]\n",
      " [  2765.96159271   -959.87072713]\n",
      " [   715.55089221  -2013.00226567]\n",
      " [  4474.58366697   1429.49697204]\n",
      " [  6712.09539718  -2205.90915598]\n",
      " [  4823.63435407  13480.55920489]]\n"
     ]
    }
   ],
   "source": [
    "# TODO: First we reduce the data to two dimensions using PCA to capture variation\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit(data)\n",
    "reduced_data = pca.transform(data)\n",
    "print reduced_data[:10]  # print upto 10 elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GMM(covariance_type='full', init_params='wmc', min_covar=0.001,\n",
      "  n_components=3, n_init=1, n_iter=100, params='wmc', random_state=None,\n",
      "  thresh=None, tol=0.001, verbose=0)\n"
     ]
    }
   ],
   "source": [
    "# TODO: Implement your clustering algorithm here, and fit it to the reduced data for visualization\n",
    "# The visualizer below assumes your clustering object is named 'clusters'\n",
    "\n",
    "use_kmeans = False\n",
    "if use_kmeans:\n",
    "    clusters = KMeans(n_clusters=3)\n",
    "else:\n",
    "    clusters = GMM(n_components=3, covariance_type='full')\n",
    "clusters.fit(reduced_data)\n",
    "print clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot the decision boundary by building a mesh grid to populate a graph.\n",
    "# print type(reduced_data)\n",
    "x_min, x_max = reduced_data[:, 0].min() - 1, reduced_data[:, 0].max() + 1\n",
    "y_min, y_max = reduced_data[:, 1].min() - 1, reduced_data[:, 1].max() + 1\n",
    "hx = (x_max-x_min)/1000.\n",
    "hy = (y_max-y_min)/1000.\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, hx), np.arange(y_min, y_max, hy))\n",
    "\n",
    "# Obtain labels for each point in mesh. Use last trained model.\n",
    "Z = clusters.predict(np.c_[xx.ravel(), yy.ravel()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-16124.38278841  -5074.17173035]\n",
      " [  4460.37052071  -3349.57116377]\n",
      " [  1718.61432297  19219.30845756]]\n"
     ]
    }
   ],
   "source": [
    "# TODO: Find the centroids for KMeans or the cluster means for GMM \n",
    "\n",
    "if use_kmeans:\n",
    "    centroids = clusters.cluster_centers_\n",
    "else:\n",
    "    centroids = clusters.means_\n",
    "print centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZMAAAELCAYAAAAcKWtPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n2v4fAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11c2efb50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Put the result into a color plot\n",
    "Z = Z.reshape(xx.shape)\n",
    "plt.figure(1)\n",
    "plt.clf()\n",
    "plt.imshow(Z, interpolation='nearest',\n",
    "           extent=(xx.min(), xx.max(), yy.min(), yy.max()),\n",
    "           cmap=plt.cm.Paired,\n",
    "           aspect='auto', origin='lower')\n",
    "\n",
    "plt.plot(reduced_data[:, 0], reduced_data[:, 1], 'k.', markersize=2)\n",
    "# plt.scatter(centroids[:, 0], centroids[:, 1],\n",
    "#             marker='x', s=169, linewidths=3,\n",
    "#             color='w', zorder=10)\n",
    "centroid_colors = ['r', 'w', 'y']\n",
    "for i in range(0, 3):\n",
    "    plt.scatter(centroids[i, 0], centroids[i, 1],\n",
    "                marker='x', s=169, linewidths=3,\n",
    "                color=centroid_colors[i], zorder=10)\n",
    "\n",
    "plt.title('Clustering on the wholesale grocery dataset (PCA-reduced data)\\n'\n",
    "          'Centroids are marked with white cross')\n",
    "plt.xlim(x_min, x_max)\n",
    "plt.ylim(y_min, y_max)\n",
    "plt.xticks(())\n",
    "plt.yticks(())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7)** What are the central objects in each cluster? Describe them as customers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Answer:**\n",
    "\n",
    "The first cluster--which takes the lower left corner--is a group of customers that buys some fresh product and less grocery, milk, and detergents paper. These seems to be smaller family owned shops selling mostly fresh product.\n",
    "\n",
    "The second cluster--which spans upper left to upper right corner--is a group of customers that buys more grocery, milk and detergents paper as well as fresh products. This group seems to be the larger grocery store like Safeway, or Kroger. There are less of these customers (maybe around 10%-15% of all customers), but their total purchase makes up a bigger proportion of the company revenue. \n",
    "\n",
    "The third cluster--which takes a small area on the lower right corner and accounts for majority of the customers and probably the majority of the company revenue--is a group of customers that buys more fresh product and less grocery, milk, and detergents paper. This group seems to be similar to the first group, but buys more fresh product. This 1st and 3rd group could be a store selling fresh product or a deli shop selling fresh food, sadwiches, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusions\n",
    "\n",
    "** 8)** Which of these techniques did you feel gave you the most insight into the data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Answer:**\n",
    "\n",
    "I feel PCA gave us the most insight into the data. First, it tells us that 95% of the variance in the data is accounted by the first 2 principal components. It also tells us that the first principal component accounts for  variance in fresh product, while the 2nd principal components accounts for variance in grocery, milk, and detergents paper. We also use PCA to reduce the data to 2 dimensions by projecting the data to the first 2 principal components. Having the data in 2 dimensions allows us to visualize the customer segments. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**9)** How would you use that technique to help the company design new experiments?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "\n",
    "We can use PCA and clusering method to divide the customers into 2 or 3 segments. When the company designs new experiments, they could conduct A/B testing where the new experiment is implemented for half of the customers in each segments, while the other half continue without the new experiment. For each customer segment we measure the customer satisfaction or increase in revenue (order) and compare the result with the baseline group (without new new experiment). This way we can see if a change affects a customer segment positively or negatively. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**10)** How would you use that data to help you predict future customer needs?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "\n",
    "We could use the data to label our existing customers into roughly 2 segments: smaller businesses buying mostly fresh product (which is the majority of the company's customer), and bigger stores buying fresh product, grocery, detergents paper, etc. We can implement a supervised learning model and train it using this existing data to predict which customer segment a new customer will fall into (after we have their purchasing data).\n",
    "\n",
    "If we have a history of purchasing data (possbily broken down to months). We can analyze the purchasing pattern for each customer segment and create a regression model to predict how much a certain customer will spend in the next month, and what kind of product they will purchase, so the company can prepare for how much stock to keep in their warehouse. \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
